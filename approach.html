<!-- <!DOCTYPE html>
<html>
  <head>
    <title>Approach Page</title>
  </head>
  <body>
    <h1>Approach</h1>
    <p>This page is dedicated to explaining our approach to solving the problem.</p>
  </body>
</html>
 -->
<!DOCTYPE html>
<html>
<head>
	<title>Approach</title>
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
	<style>
		header {
			background-color: #333;
			color: #fff;
			padding: 10px;
			display: flex;
			align-items: stretch;
			justify-content: space-between;
		}
		
		.links {
			display: flex;
			align-items: center;
			justify-content: space-between;
			flex-grow: 1;
		}
		
		.links a {
			color: #fff;
			text-decoration: none;
			padding: 10px;
		}
		
		main {
			display: flex;
			flex-direction: column;
			align-items: center;
			justify-content: center;
			height: 500px;
		}
		
		.container {
			text-align: center;
		}

		footer {
			background-color: #333;
			color: #fff;
			padding: 5px;
			text-align: center;
		}
	</style>
</head>
<body>
	<header>
		<div class="links">
			<a href="index.html">Home</a>
			<a href="problem.html">Problem Statement</a>
			<a href="approach.html">Approach</a>
			<a href="evaluation.html">Evaluation</a>
			<a href="conclusion.html">Conclusion</a>
		</div>
	</header>
	
<!-- 	<main>
		<img src="ISR.jpg" alt="My Image">
	</main> -->
	
	<div class="content">
	<h1>Approach</h1>
    <p>Let R<sub>cp</sub>, R<sub>cv</sub> and R<sub>like</sub> be binary relationships where, aR<sub>cp</sub>b represents that a is co-purchased with b, aR<sub>cv</sub>b represents that a is coviewed with b, and aR<sub>like</sub>b represents that a is similar to b based on its product features.</p>
	
	<div class="container">
		<img src="ISR.jpg" alt="Description of image">
	</div>
	
	<p>Consider a sample product graph in Figure 1, where a node corresponds to a product and an edge corresponds to a co-purchase or co-view relationship between the products.RPR problem entails few challenges which we illustrate using Figure 1 as follows:
		<ul>
			<li>Product Relevance: Given a query product P, we would like to suggest related products such as an adapter AC, phone case PC, AirPods AP, screen guard SC</li>
			<li>Product Asymmetry: For a phone P, we would like to recommend a suitable phone case PC, but for a phone case PC, it may not be apt to recommend a phone P because customers typically would purchase a phone case only while owning a phone. Formally, product asymmetry can be represented as \[aR_{cp}b \not \Rightarrow bR_{cp}a\]</li>
			<li>Selection Bias: It is inherent to historical purchase data due to several factors like product availability, price etc. For example, while shopping for a phone case for phone P, customer c<sub>1</sub> might browse phone cases PC and PC1 (PC2 and PC3 are not shown to him owing to some of the factors mentioned above). Another customer c<sub>2</sub> might browse phone cases PC and PC2 (PC1 and PC3 are not presented to him). A third customer c<sub>3</sub> might browse phone cases PC and PC3 (PC1 and PC2 are not presented to him). Assume that all three customers eventually purchase PC; this could also be due to a bias in the list of products shown to each customer. In our example, across different customers, PC, PC1, PC2 and PC3 are co-viewed (similar). In order to correct for selection bias, we would like to recommend not only phone case PC, but also PC1, PC2 and PC3 as related products given a query product P. Formally, we want to uncover relationships of the form:
					\[aR_{cp}b \land bR_{cv}c \Rightarrow aR_{cp}c\]
					\[aR_{cv}b \land bR_{cp}c \Rightarrow aR_{cp}c\]
			</li>
			<li>Cold-Start Product Recommendations: We not only need to suggest recommendations for existing products, but also suggest recommendations for newly launched i.e. coldstart products. Formally, given two existing products a and b, and a cold-start product c, we want to uncover relationships of the form:
					\[cR_{like}a \land aR_{cp}b \Rightarrow cR_{cp}b\]
					\[aR_{cp}b \land bR_{like}c \Rightarrow aR_{cp}c\]
			</li>
		</ul>
	</p>
	<p>These rules correspond to the case when c is the query product and the recommended product respectively. Observe that mitigating selection bias and tackling cold-start products deal with modelling transitive relationships, while preserving edge asymmetry.</p>
	<h1> Product Graph construction </h1>
	<p>We had leveraged the co-purchase data CP and co-view data CV to create the product graph G. We represent each product by an input feature based on its metadata from catalog (product name, product description, product type etc.). In order to jointly model product co-purchase likelihood and product asymmetry, we represent each product using dual i.e. source and target embeddings. We train our model using an asymmetric loss function. The trained model generates product embeddings by appropriately aggregating features from its neighborhood in G. For every node u in G, we generate a pair of embeddings (denoted by u-s and u-t). In order to address node asymmetry, we use the following rationale: 
	<ul>
		<li>while generating the source embedding of a node, we aggregate information from the target embedding of its out-neighbors</li>
		<li>while generating the target embedding of a node, we aggregate information from the source embedding of its in-neighbors. </li>
	</ul>
	Compared to undirected graphs, the computational graph corresponding to the source and target embedding of each node is different in directed graphs. Consequently, this trick enables us to model edge strength and edge direction efficiently in directed graphs. Given a query product q, we generate its source embedding and perform a nearest neighbor lookup in the target embedding space to recommend top-k related products for q. </p>
	
	<p>We had generated the product embeddings assuming that model is already trained. The first step involves describing the forward pass of the model, which generates the product embeddings. Once the embeddings are generated, the next step is to learn the model parameters using stochastic gradient descent and backpropagation techniques.</p>
	
	<h1>Related product recommendation</h1>
	<p>Given a query product q, we use θ<sub>q</sub><sup>s</sup>, the source embedding of q, to perform a nearest neighbor lookup in the target embedding space of all the products to recommend top-k set of related products denoted by R<sub>k</sub><sup>q</sup>. Specifically, for a query product q &isin; P, we compute a relevance score with respect to a candidate product v &isin; P as shown below
	
		\[rel(q,v) = (\theta_q^s)^T \times \theta_v^t\]
	
	Observe that rel(q, v) is not equal to rel(v, q) which helps capturing product asymmetry
	</p>
	
	<h1>Implementation Details</h1>
	<p>
	We have implemented it using PyTorch. We vary the learning rate in {10<sup>-1</sup> , 10<sup>-2</sup> , 10<sup>-3</sup> , 10<sup>-4</sup>} and observed 10<sup>-4</sup> using Adam’s optimizer to work the best. We use L = 3 layer GNN model for this application. Our data set had approximately 1000 nodes. After generating the product embeddings, we perform nearest neighbor lookup to suggest top-k related products for k values in {5, 10, 20}. For all models, we learn 64 dimensional product embeddings trained for a maximum of 30 epochs. We repeat all the experiments 10 times and report the average value across the runs
	</p>
	</div>
	
	<footer>
		<p> That is it about Product Rec :) </p>
	</footer>
</body>
</html>
